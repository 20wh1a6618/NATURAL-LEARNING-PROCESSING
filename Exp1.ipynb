{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/20wh1a6618/NATURAL-LEARNING-PROCESSING/blob/main/Exp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8WsDi1RLAHP"
      },
      "source": [
        "Word Analysis\n",
        "\n",
        "Regular Expression(RegEx)\n",
        "\n",
        "A RegEx is a sequence of characters that forms a search pattern.RegEx can be used to check if a string contains the specified search pattern.\n",
        "\n",
        "\n",
        "Why are regular expressions essential for NLP?\n",
        "Whenever we deal with text data, it may have words we want to remove,punctuation taht is not needed,hyperlinks or HTML that can be done away with, and dates or numerical entities that can be made simpler.\n",
        "\n",
        "Python has a built in package called re,which can be used to work with regular expressions.\n",
        "\n",
        "Import the re module:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCyxr895s4oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMipy_86JeEz"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33FV-iptMP2Z"
      },
      "source": [
        "The findall() function returns a list containing all matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EztiuiyJ0mO",
        "outputId": "cbec5268-65d5-4dc4-fc8f-1a6b45f0a955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai', 'ai']\n"
          ]
        }
      ],
      "source": [
        "txt=\"The rain in spain\"\n",
        "x=re.findall(\"ai\",txt)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMORS744MST3"
      },
      "source": [
        "The list contains the matches in the order they are found.If no matches are found, an empty list is returned.\n",
        "\n",
        "\n",
        "2. The search() function searches the string for a match,and returns a Match object if there is a match.If there is more than one match ,only the first occurence of the match will be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jBJtz4CHM8E8",
        "outputId": "0f9d401f-626e-42db-a07d-9fd25fee383b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first white-space character is located in position: 3\n"
          ]
        }
      ],
      "source": [
        "txt=\"The rain in spain\"\n",
        "x=re.search(\"\\s\",txt)\n",
        "print(\"The f\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "puK3CfqF2Hdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jybeLzgy2N7B",
        "outputId": "bdfe0051-4959-4ee1-a719-98c5daab50d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n"
      ],
      "metadata": {
        "id": "vpd4u5S92X77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tknzr=TweetTokenizer()"
      ],
      "metadata": {
        "id": "s0VGTZJb2n_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s0=\"This is cool\""
      ],
      "metadata": {
        "id": "TeBOrnAk24ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S1=\"This is coo1!\""
      ],
      "metadata": {
        "id": "au_1VRz23C4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tknzr.tokenize(s0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfkG45u23LZp",
        "outputId": "93301d46-a81c-4676-f022-be1ae7b136d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'cool']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tknzr.tokenize(S1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lQtkXwn3nqj",
        "outputId": "e643b3aa-56ab-4c58-fcc4-8f0c0aa58d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'coo', '1', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpDttAiGK_Jg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk\n",
        " nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AZ6UGuahvFk",
        "outputId": "bba83bf1-784d-46ec-e416-05d3b8125bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "spPHvTqM44Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbO5-25a5QwD",
        "outputId": "d8e0751a-6fbb-4d11-cbd1-87229acf6ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"wouldn't\", 'all', 'same', 'into', 'no', 'by', 'being', 'under', 'very', \"isn't\", 'their', 'those', \"mightn't\", 'against', 'can', \"hadn't\", 'after', 'should', 'ma', 've', 'wasn', 'has', 'have', 'during', 'yourselves', 'because', 'she', 'on', 'will', 'am', \"doesn't\", 'you', \"haven't\", 'are', 're', 'that', \"didn't\", \"it's\", 'between', 'theirs', 'at', 'again', 'he', 'not', 'up', 'did', 'to', 'what', 'but', 'than', 'for', 's', 'why', 'now', 'few', 'our', 'which', 'i', 'haven', 'they', 'itself', 'about', \"couldn't\", 'how', 'whom', 'me', 'my', 'them', 'then', \"needn't\", 'couldn', 'o', \"she's\", 'him', 'some', 'ourselves', 'his', \"shouldn't\", 'do', 'won', 'both', 'was', 'a', 'over', 'each', 'it', 'when', 'had', \"shan't\", 'is', \"you're\", 'an', \"aren't\", 'own', 'having', \"you'll\", 'we', 'its', 'were', 'this', 'down', \"don't\", 'll', 't', 'hers', 'only', 'shouldn', 'isn', 'with', 'themselves', 'hadn', 'until', 'once', 'doesn', 'your', 'wouldn', 'yours', 'needn', 'nor', 'm', 'herself', 'here', 'who', 'in', 'from', 'ain', 'weren', 'does', 'where', 'any', 'or', 'too', \"that'll\", 'above', \"won't\", 'and', 'himself', \"should've\", \"you've\", 'there', 'her', \"weren't\", 'most', 'just', 'such', 'doing', 'been', 'y', \"you'd\", 'be', 'mightn', 'more', 'ours', 'hasn', 'd', 'other', 'don', 'myself', 'so', 'didn', 'yourself', 'before', 'of', 'mustn', 'shan', 'further', \"hasn't\", \"mustn't\", 'the', 'as', \"wasn't\", 'aren', 'through', 'while', 'out', 'if', 'these', 'off', 'below'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentence = [w for w in tknzr.tokenize(s0) if not w in stop_words]\n",
        "\n",
        "filtered_sentence = []"
      ],
      "metadata": {
        "id": "EhfyUvjI6Fub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in tknzr.tokenize(s0):\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)"
      ],
      "metadata": {
        "id": "cdmBd_vi64tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tknzr.tokenize(s0))\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP1nKjXo7RMp",
        "outputId": "a38624f2-3be8-46e0-9ee0-4f893eff1dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'cool']\n",
            "['This', 'cool']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        " \n"
      ],
      "metadata": {
        "id": "BwpD5hc-s71-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization\n",
        "\n",
        "\n",
        "During the tokenization process, phrases and words split together. The sentence separates into tokens that are divided into characters that include text and punctuation.\n",
        "\n",
        " \n",
        "\n",
        "To make it easier for the computer to understand, the languages of English and NLP techniques work differently. The segmented languages provide the information of each block that helps the machine in evaluating the meaning of words.\n",
        "\n",
        " \n",
        "\n",
        "Tokenization can remove punctuation marks. Otherwise, punctuation granted a separate token.\n",
        "\n",
        " \n",
        "\n",
        "Tokenization example with Spacy"
      ],
      "metadata": {
        "id": "KFWtu_9bt8A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The way to get started is to quit talking and begin doing.\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUaJ0iyqtTaO",
        "outputId": "e120bfbf-b2ce-4be2-b9d4-b7895c818242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "way\n",
            "to\n",
            "get\n",
            "started\n",
            "is\n",
            "to\n",
            "quit\n",
            "talking\n",
            "and\n",
            "begin\n",
            "doing\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization example with NLTK"
      ],
      "metadata": {
        "id": "BI2JBQGHuHsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The way to get started is to quit talking and begin doing.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF9BXDTRt5IX",
        "outputId": "c6174855-19f2-4ec0-9813-159dc69a725f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'way', 'to', 'get', 'started', 'is', 'to', 'quit', 'talking', 'and', 'begin', 'doing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop Word Removal\n",
        "\n",
        "\n",
        "In the English language, the stop words are prepositions, pronouns, and articles. Stop words are very usual in sentences, although they have little significance for natural language processing systems. As a result, the objects are filtered out, and common keywords that provide no information about the text removed.\n",
        "\n",
        " \n",
        "\n",
        "The pre-defined keywords list deletes stop words, which also frees up space in the database. However, there is no usual list of stop words. They are created or pre-selected to meet the software’s requirements. Stopword removal is, in my opinion, one of the most popular NLP techniques out there. You will perform it in almost all your NLP projects.\n",
        "\n",
        " \n",
        "\n",
        "Stop Word Removal Example with NLTK"
      ],
      "metadata": {
        "id": "FzmUe7OUuSln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Get your text \n",
        "text = \"The way to get started is to quit talking and begin doing.\"\n",
        " \n",
        "# Tokenize your text \n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove text by checking if token is in the stopwords list \n",
        "processed_text = [w for w in tokens if not w.lower() in stop_words]\n",
        " \n",
        "print(tokens)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz1A9xtluMaI",
        "outputId": "3ad9dbd3-dddb-4233-822a-fc830dea0494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'way', 'to', 'get', 'started', 'is', 'to', 'quit', 'talking', 'and', 'begin', 'doing', '.']\n",
            "['way', 'get', 'started', 'quit', 'talking', 'begin', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming & Lemmatization"
      ],
      "metadata": {
        "id": "jv-s4zgoukWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is just another feature of NLP to be aware of. The process of extracting the root of the word is known as stemming. In daily speech, removing prefixes and suffixes at the beginning or end of each word is referred to as stemming.\n",
        "\n",
        " \n",
        "\n",
        "However, some typical complications arise when expanding or creating a new word known as inflectional affixes and derivational affixes, respectively. As a result, NLP programming languages such as R and Python works in connection with different tools to guarantee that the steaming process is as simple as possible.\n",
        "\n",
        "There are various stemmers that you can use to perform stemming. The most popular stemmers type are Porter’s stemming, Snowball Stemming, Dawson stemming, Lovins stemming, N-Gram Stemming, Krovetz stemming, Lancaster stemming, Xerox stemming.\n",
        "In comparison, Lemmatization reduces the shape of a word or combines similar words. In the standardizing words, for example, tenses, synonyms, and so on are put together based on their meaning. Both Lemmatization & Stemming are NLP techniques used to preprocess text.\n",
        "\n",
        " \n",
        "\n",
        "How to perform Stemming with NLTK?\n",
        "Spacy does not have a stemming library. But, NLTK does have both a stemming and a lemmatization library. "
      ],
      "metadata": {
        "id": "-aUDMwozuoJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "text = \"The way to get started is to quit talking and begin doing.\"\n",
        "# Tokenize your text \n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "processed_text =  [stemmer.stem(w) for w in tokens]\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYGkoID7umfU",
        "outputId": "e33819de-d75d-4d30-cd0e-6f051d928c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'way', 'to', 'get', 'start', 'is', 'to', 'quit', 'talk', 'and', 'begin', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to perform Lemmatization with NLTK & Spacy?\n",
        "Lemmatization with NLTK"
      ],
      "metadata": {
        "id": "zFmeiAYTviQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VG_nHdCvuTx",
        "outputId": "0d702911-d53f-4b1e-9fea-7daba35ad3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text = \"take takes taking taken eat eaten ate\"\n",
        "# Tokenize your text \n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "for token in tokens:\n",
        "    print(\"{} : {} \".format(token, lemmatizer.lemmatize(token, \"v\" )))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxcdzKQxvlAZ",
        "outputId": "b90873d3-eace-4693-8d6c-378a1588551f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take : take \n",
            "takes : take \n",
            "taking : take \n",
            "taken : take \n",
            "eat : eat \n",
            "eaten : eat \n",
            "ate : eat \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization with Spacy"
      ],
      "metadata": {
        "id": "-ODgFHzNv6zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Take takes taking taken eat eaten ate\")\n",
        "for token in doc:\n",
        "    print(token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h_GkY8qv7yO",
        "outputId": "4c06b584-ad7f-455f-e196-995c9a7cce47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take\n",
            "take\n",
            "taking\n",
            "take\n",
            "eat\n",
            "eat\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Modeling\n",
        "\n",
        "\n",
        "\n",
        "The hidden structure of documents and text is revealed by analyzing individual words and contents and then assigning values to them. It is one of the other popular NLP techniques. On the other hand, the assumption is a critical part of this approach in which themes are blended based on the words.\n",
        "\n",
        " \n",
        "\n",
        "Once the text distributes, the hidden content reveals using natural language processing techniques to determine the exact meaning of the text. Latent Dirichlet Allocation (LDA) came into existence around two decades ago as a subject modelling methodology based on unsupervised learning. In this case, the learning process is based on a single output variable, and algorithms are used to examine the data and discover the pattern. LDA uses the related terms group as follows.\n",
        "\n",
        " \n",
        "\n",
        "It generates random topics and assigns numbers to them based on what you want to learn. These random topics are specified as integers that map out using an algorithm to locate the words in the text.\n",
        "\n",
        " \n",
        "\n",
        "The natural language processing system scans each word, assessing the possibility and reassigning the terms to the topic. Multiple scans are performed, and possibilities evaluate until the algorithms conclude.\n",
        "\n",
        " \n",
        "\n",
        "In comparison to the K-means algorithm, LDA works on a variety of topics in the document. This process makes the conclusions more realistic and better explains the issue. With this information, you may conclude that the time has come to invest in AI application development.\n",
        "\n",
        " \n",
        "\n",
        "Example of Topic Modelling using LDA with Python"
      ],
      "metadata": {
        "id": "TQBqT_G_wG81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stop-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zel6MZ3JwuxA",
        "outputId": "b3e8b0f3-9751-48ac-e97d-cef7e42fc808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32909 sha256=6e1f161d2f0f7bcb49ae6cf3f88e98c022f8eeda44bee9c54ee4ec67f00184c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/d8/66/395317506a23a9d1d7de433ad6a7d9e6e16aab48cf028a0f60\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stop_words import get_stop_words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "0EhEl5Lcwd_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Import your documments, these can be paragraphs, full text, sentences etc\n",
        "# These documents are basically your data which you have to import appropriatelly\n",
        "# For the sake of simplicity I will write my documents out bus I suggest you\n",
        "# Not to go the same. Import them appropriatelly.\n",
        "\n",
        "a = \"French comedy film won at the Canne festival\"\n",
        "b = \"The food cooked by the Italian Chef tasted fantastic\"\n",
        "c = \"The Avenger movie was a piece of art. The film was very captivating\"\n",
        "d = \"The Covid transmission rate increase by 68%, said the National Health Institute\"\n",
        "e = \"French cuisine has one of most sophisticated type of food in the world\"\n",
        "\n",
        "# compile sample documents into a list\n",
        "docs = [a, b, c, d, e]\n",
        "\n",
        "\n",
        "# Save all the tokens in the various document in one place\n",
        "text = []\n",
        "\n",
        "# create a list of stop words\n",
        "stopwords = get_stop_words('en')\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# loop through docs \n",
        "for i in docs:\n",
        "    \n",
        "    # clean and tokenize document string\n",
        "    temp = i.lower()\n",
        "    tokens = word_tokenize(temp)\n",
        "\n",
        "    # Removing stopwords \n",
        "    processed_text = [w for w in tokens if not w.lower() in stopwords]\n",
        "    # Stemming\n",
        "    processed_text = [stemmer.stem(w) for w in processed_text]\n",
        "    \n",
        "    # add tokens to list\n",
        "    text.append(processed_text)\n",
        "\n",
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "final_data = corpora.Dictionary(text)\n",
        "    \n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [final_data.doc2bow(docs) for docs in text]\n",
        "\n",
        "# generate LDA model\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = final_data, passes=20)\n",
        "\n",
        "# View the results\n",
        "print(ldamodel.print_topics(num_topics=3, num_words=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS4SrldGwIni",
        "outputId": "4611bf4a-0ffc-40a8-d328-17e1d8a25f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.073*\"film\" + 0.073*\"movi\" + 0.073*\"captiv\"'), (1, '0.066*\"french\" + 0.038*\"said\" + 0.038*\"transmiss\"'), (2, '0.078*\"food\" + 0.077*\"cook\" + 0.077*\"italian\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the above we can see and infer that group 1 sentences are about food, group 2 about movies, and group 3 about health. It makes sense since the sentences mentioned that general category.  This technique works great if you have, let’s say, a thousand articles from a website and you want to quickly know what the website is about. You can potentially use the above technique to extract the top N categories/topics. "
      ],
      "metadata": {
        "id": "hDBaHyXKxLwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings\n",
        "\n",
        "\n",
        "It is one of the fundamental NLP techniques that use vector representation to describe tokens (converted to numbers). Vector patterns are applied to analyze numbers to solve this.\n",
        "\n",
        " \n",
        "\n",
        "This method captures the spirit of the words and demonstrates the connection between an actual number and the tokens. A vector with a length of 100 represents the fixed dimension. \n",
        "\n",
        " \n",
        "\n",
        "How to create word Embeddings with Python?\n",
        "The code snippet below is the most basic way to create word embeddings using Word2Vec. We imported a sample text from the NLTK library, then we fitted a Word2Vec model on the sentences in that text. And based on that Word2Vec model, we are able to identify want words in the text that are more similar to a term we input. This technique works great if you do search recommendations or even keyword research in Marketing.\n",
        "\n"
      ],
      "metadata": {
        "id": "T5dfMpUZxSYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('abc')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ckXvPaxdpL",
        "outputId": "9c1bfe9d-1d17-44c9-82a7-cb5c7db301a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/abc.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import abc\n",
        "#nltk.download('abc')\n",
        "model= gensim.models.Word2Vec(abc.sents())\n",
        "words= list(model.wv.vocab)\n",
        "res=model.wv.most_similar('mobile')\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_xyhz_hxNPi",
        "outputId": "0b4b15be-2006-4af1-c1f2-9c98770e1cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('phones', 0.9509194493293762), ('devices', 0.9492298364639282), ('traffic', 0.9478411674499512), ('solutions', 0.9455376863479614), ('input', 0.9444797039031982), ('inhibit', 0.9420850276947021), ('care', 0.9406677484512329), ('systems', 0.9395544528961182), ('engines', 0.938389778137207), ('upside', 0.9364399313926697)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c3ohudY1xa9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Disambiguation & Named Entity Recognition\n",
        "\n",
        "\n",
        "\n",
        "The sentence’s entities are recognized in disambiguation, such as the name of a famous person, brand, country etc. For example, the news highlights a new product released by Apple. With NLP approaches in AI, named entity disambiguation is used to assume that Apple is the brand here rather than fruit.\n",
        "\n",
        " \n",
        "\n",
        "In comparison, named entity recognition identifies and categorizes the entity based on the date, organization, person, time, place, etc. Here’s how you perform NER with Python."
      ],
      "metadata": {
        "id": "A7Spmtdfx1TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Mark Zuckerberg, CEO of Facebook, is looking at buying U.K. startup for $500 million\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9aqXU95x5wr",
        "outputId": "26085bff-79e3-4f6e-a272-9ac20ba3b2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mark Zuckerberg PERSON\n",
            "Facebook ORG\n",
            "U.K. GPE\n",
            "$500 million MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using an arbitrary sentence, the NLP system successfully managed to identify Mark Zuckerberg as a PERSON, Facebook as an Organization, UK as a Geopolitical Entity (Country, State, Cities, etc) and 500 million as Monetary value. NER is a great way to filter through text. For example, it works great if your want to know how many times a particular type of entity is explicitly mentioned in a text. "
      ],
      "metadata": {
        "id": "516XFm1JyOFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language Identification & Text Summarization:\n",
        "\n",
        "\n",
        "\n",
        "Language identification uses syntax and statistical features to identify the language based on its content. Text summarization, on the other hand, is comparable to language identification. Still, it shortens the recognized text, making it an essential aspect of natural language processing training for beginners.\n",
        "\n",
        " \n",
        "\n",
        "Here is how you can identify a language with Python."
      ],
      "metadata": {
        "id": "wOB4ApNKyTaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mrG-E2Iyi5k",
        "outputId": "8c996394-077f-4abf-e9eb-f8dca9df5a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993244 sha256=f764301aa90d5112487acea0d518626a717c6fec9361f145e8c4a438231465ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "french = \"J'ai mis ma pomme dans le frigo\"\n",
        "english = \"I loved my time in New York\"\n",
        "italian = \"Voglio mangiare una pizza\"\n",
        "\n",
        "test = [french, english, italian]\n",
        "for language in test:\n",
        "    print(detect(language))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUMuR1c0yPaT",
        "outputId": "18d438d0-2474-4a89-e7ad-6f978f4dd9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fr\n",
            "en\n",
            "it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Benefits of Using Natural Language Processing (NLP Techniques)\n",
        "\n",
        "\n",
        "\n",
        "To adopt and learn words and grammar, natural language processing (NLP) use machine learning (ML) techniques. After that, the inputs process using grammatical rules, linguistic habits, and regular algorithms to generate computer-based natural language. The approach helps in the translation of languages.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "Every internet user has used a natural language processing (NLP) program. Search engines such as Google and Bing use Natural Language Processing to suggest possible search requests. When users begin to enter search parameters, search engines attempt to finish the demand for them. Users can choose from the recommended criteria or continue typing their query.\n",
        "\n",
        " \n",
        "\n",
        "NLP uses are not limited to search engines. Voice-activated devices use NLP such as Siri and Alexa to assist in process language. Indeed, chatbots use NLP to provide more accurate replies to end-user inquiries. The technique may be used to extract relevant information from unstructured data to create better data sets. Furthermore, there are numerous significant advantages of using NLP in corporations.\n",
        "\n",
        " \n",
        "\n",
        "Offer Immediate Customer Service:\n",
        "Even if you’ve never heard of Natural Language Processing, we’re guessing you’ve heard of chatbots — AI-powered software that can converse with users via websites or applications. Chatbots deploy NLP to provide your consumers with rapid replies to any issue, no matter what time of day or week it is.\n",
        "\n",
        " \n",
        "\n",
        "Because chatbots fulfil the customer service executive job, if inquiries focus on the same topics, chatbots might employ predetermined replies to save clients ever waiting for a service desk response. Furthermore, they can give special assistance, such as providing a link to instructions, reserving a service, or locating different items.\n",
        "\n",
        " \n",
        "\n",
        "Chatbots can now recognize a user’s intent owing to developments in machine learning algorithms and word analysis. Chatbots are the ideal answer in an age where the immediate reward is expected. They’ll even transform possibilities into customers by providing exceptional service.\n",
        "\n",
        " \n",
        "\n",
        "As you can see, automated customer service has numerous advantages. According to Opus analysis, firms would invest up to $4.5 billion in chatbots by 2021.\n",
        "\n",
        " \n",
        "\n",
        "Better Data Analysis:\n",
        "When doing repeated jobs, such as reading and analyzing open-ended survey replies and other text data, people are likely to make mistakes or having flaws that might affect the results.\n",
        "\n",
        " \n",
        "\n",
        "NLP-powered tools may be trained to your company’s language and requirements in a matter of minutes. As a result, once they’re up and running, they execute far more accurately than humans ever could. And, as your business’s marketplace or language changes, you may fine-tune and train your systems.\n",
        "\n",
        " \n",
        "\n",
        "Streamlined Processes\n",
        "Many professional service businesses, such as legal firms or accounting firms, must evaluate massive amounts of financial material. Developing a natural language processing solution tailored to the needs of legal and accounting professionals can minimize the amount of time spent searching for specific sections. \n",
        "\n",
        " \n",
        "\n",
        "Because many agreements have identical language, workers might spend hours searching for the correct document. A chatbot may be trained using NLP to discover specific clauses across numerous contracts without the need for human interaction.\n",
        "\n",
        " \n",
        "\n",
        "Using a chatbot to create and review contracts simplifies the process. It also allows employees to work on other projects while papers are being searched. NLP can increase efficiency in areas other than professional services. \n",
        "\n",
        " \n",
        "\n",
        "Chatbots can assist customer service representatives in answering queries quickly. Employees can implement NLP to search across numerous sources and deliver details with faster response times rather than manually searching a knowledge base or helpdesk.\n",
        "\n",
        " \n",
        "\n",
        "Reduce Costs And Inefficiencies\n",
        "Running a profitable business involves reducing expenses wherever possible. While everyone wants to increase the amount of money that comes into their firm, simplifying your current operation by enhancing overall efficiency may do wonders for your profit statement.\n",
        "\n",
        " \n",
        "\n",
        "NLP-trained chatbots can significantly minimize the expenses associated with manual and repetitive operations. While there are now opportunities for savings, companies will profit much more in the future as machine learning enhances chatbot capability and consumers get more comfortable interacting with robots. \n",
        "\n",
        " \n",
        "\n",
        "Empowered Employees\n",
        "Employees may do higher-level jobs when repetitious functions eliminate. It reduces activities that contribute to boredom, tiredness, and disengagement. The use of NLP technology can result in a more successful organization.\n",
        "\n",
        " \n",
        "\n",
        "NLP solutions empower employees. An NLP chatbot can assist workers in efficiently obtaining information. The technology can produce a more comprehensive data set since it processes data from many sources. Employees can interpret the data to respond to client requests or to complete assigned work more efficiently. They are not required to waste time looking through files.\n",
        "\n",
        " \n",
        "\n",
        "Giving employees the freedom to work independently improves staff happiness and engagement. Employees who engaged are better representatives for a brand and provide better customer experiences, resulting in a high level of customer satisfaction.\n",
        "\n",
        " \n",
        "\n",
        "Future of NLP\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "NLP is becoming a fundamental aspect of technology in the modern-day, particularly with machine learning, deep learning, and artificial intelligence solutions at its foundation. Companies now can communicate with customers more readily and make a difference in the market by learning from them.\n",
        "\n",
        " \n",
        "\n",
        "Companies apply natural language processing (NLP) approaches to improve consumer interactions, interact with data, and achieve the desired conclusion. Natural language processing (NLP) tools are assisting in making procedures better and quicker. It brings a new era of communication with machines; companies are now making more informed decisions and becoming more adaptable. It is a fundamental revolution in market technology while keeping client feelings in mind.\n",
        "\n",
        " \n",
        "\n",
        "Organizations will grow elegant as a result of NLP and the popularization of intelligence in a way that will benefit them. Integrating NLP with other technologies will change how consumers interact with technology such as computers and smartphones. When it comes to NLP, the future is looking bright, and advances will make the future even shining. \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "b-UOHsxizApd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjrG0XRgzCOm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}